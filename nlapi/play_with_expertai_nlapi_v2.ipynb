{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img style=\"float: right;\" src=\"https://docs.expert.ai/logo.png\" width=\"150px\">\n",
    " \n",
    "# My first Notebook with expert.ai Natural Language API v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **expert.ai Natural Language API v2** (https://developer.expert.ai/) parses and \"understands\" large volumes of text.\n",
    "\n",
    "In this section we'll install and play with expert.ai Natural Language API to work with Python, and then introduce some concepts related to Natural Language Processing.\n",
    "\n",
    "You can also download the source code of our Python SDK and this notebook from Github at https://github.com/therealexpertai/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "First, install __expert.ai-nlapi__ library using pip. \n",
    "* https://pypi.org/project/expertai-nlapi/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U expertai-nlapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, you're ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with NL API in Python\n",
    "First you have to setup your account credentials; if you don't have them, get them at https://developer.expert.ai/ui/login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your environment variables with NL API credentials \n",
    "\n",
    "```bash\n",
    "SET EAI_USERNAME=YOUR_USER\n",
    "SET EAI_PASSWORD=YOUR_PASSWORD\n",
    "```\n",
    "or \n",
    "\n",
    "```bash\n",
    "export EAI_USERNAME=YOUR_USER\n",
    "export EAI_PASSWORD=YOUR_PASSWORD\n",
    "```\n",
    "\n",
    "as an alternative you can always add to your notebook the following statements\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"EAI_USERNAME\"] = 'YOUR_USER'\n",
    "os.environ[\"EAI_PASSWORD\"] = 'YOUR_PASSWORD'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play with Python and Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the API supports five languages i.e. English, French, Spanish, Italian and German. You have to define the text you want to process and the language model to use for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expertai.nlapi.cloud.client import ExpertAiClient\n",
    "client = ExpertAiClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Facebook is looking at buying an American startup for $6 million based in Springfield, IL .' \n",
    "language= 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick run\n",
    "Let's start with the fist API, just sending the text. This is how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `disambiguation` analysis returns all the information generated by the Natural Language engine from the text. Let's see in the details the available metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Lemmatization\n",
    "Lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{\"TOKEN\":{20}} {\"LEMMA\":{8}}')\n",
    "\n",
    "for token in document.tokens:\n",
    "    print (f'{text[token.start:token.end]:{20}} {token.lemma:{8}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part of Speech \n",
    "We also looked at the part-of-speech information assigned to each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{\"TOKEN\":{18}} {\"PoS\":{6}}')\n",
    "\n",
    "for token in document.tokens:\n",
    "    print (f'{text[token.start:token.end]:{18}} {token.pos:{6}} ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing information\n",
    "The dependency parsing information are available for each token, together with the information about the connected tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{\"TOKEN\":{18}} {\"Dependency label\":{8}}')\n",
    "\n",
    "for token in document.tokens:\n",
    "    print (f'{text[token.start:token.end]:{18}} {token.dependency.label:{4}} ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities\n",
    "Going a step beyond tokens, *named entities* add another layer of context.  Named entities are accessible through the `entities` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'entities'})\n",
    "\n",
    "\n",
    "print (f'{\"ENTITY\":{20}} {\"TYPE\":{10}}')\n",
    "       \n",
    "for entity in document.entities:\n",
    "    print (f'{entity.lemma:{20}} {entity.type_:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can get the open data connected with an entity, i.e `Springfield, IL` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document.entities[1].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in document.knowledge:\n",
    "    if (entry.syncon == document.entities[1].syncon):\n",
    "            for prop in entry.properties:\n",
    "                print (f'{prop.type_:{12}} {prop.value:{30}}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Springfield has been recognized as [Q28515](https://www.wikidata.org/wiki/Q28515) on Wikidata, that is the Q-id for Springfield, IL (i.e.not for Springfield in Vermont o in California)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Elements\n",
    "*Key elements* are identified from the document as main sentences, main keywords, main lemmas and relevant topics; let's focus on the main lemmas of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'relevants'})\n",
    "\n",
    "\n",
    "print (f'{\"LEMMA\":{20}} {\"SCORE\":{5}} ')\n",
    "       \n",
    "for mainlemma in document.main_lemmas:\n",
    "    print (f'{mainlemma.value:{20}} {mainlemma.score:{5}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Let's see how to classify documents according the **IPTC Media Topics Taxonomy**; we're going to use a text that has more textual information and then we'll use the matplot lib to show the categorization result\n",
    "* [taxonomy definition](http://cv.iptc.org/newscodes/mediatopic)\n",
    "\n",
    "Results will be displayed using Matplotlib, so first install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.4)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.4.2-cp38-cp38-win_amd64.whl (7.1 MB)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.4\n",
      "    Uninstalling matplotlib-3.3.4:\n",
      "      Successfully uninstalled matplotlib-3.3.4\n",
      "Successfully installed matplotlib-3.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\avarone\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, set the text you want to classifiy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Britain gave emergency approval on Wednesday to Pfizer’s American-developed coronavirus vaccine, leaping ahead of the United States to become the first Western country to allow its health service to begin mass inoculations against a disease that has killed more than 1.4 million people worldwide.\n",
    "The approval kicks off a vaccination campaign with little precedent in modern medicine, encompassing not only ultracold dry ice and trays of glass vials but also a crusade against anti-vaccine misinformation.\n",
    "The specter of Britain beating the United States to approval had already angered the White House in recent days, heaping additional pressure on American regulators to match Britain’s pace.\n",
    "But while the go-ahead for Pfizer bodes well for rich countries like Britain that have ordered tens of millions of doses, it offered little relief to poorer countries that could not afford to buy supplies in advance and may struggle to pay for the exceptional demands of distributing the vaccine.\n",
    "Already, the quandary of transporting vials at South Pole–like temperatures was dictating who could be vaccinated: Nursing-home residents were supposed to be Britain’s top priority under an advisory committee’s plans, but a limit on how many times officials believe the Pfizer vaccine can be moved before it loses effectiveness means that National Health Service staff members will receive the shots first.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "iptc_classification = client.classification(body={\"document\": {\"text\": text}}, params={'taxonomy': 'iptc', 'language': language})\n",
    "\n",
    "iptc_categories = []\n",
    "iptc_scores = []\n",
    "\n",
    "print (f'{\"CATEGORY\":{27}} {\"ID\":{10}} {\"FREQUENCY\":{8}}')\n",
    "for category in iptc_classification.categories:\n",
    "    iptc_categories.append(category.label)\n",
    "    iptc_scores.append(category.frequency)\n",
    "    print (f'{category.label:{27}} {category.id_:{10}}{category.frequency:{8}}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(iptc_categories, iptc_scores, color='#17a2b8')\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Media Topics Classification\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NL API v2 introduced an additional classifier, that classifies documents according to a geographic taxonomy.\n",
    "* [taxonomy definition](https://docs.expert.ai/nlapi/latest/guide/taxonomies/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "geo_classification = client.classification(body={\"document\": {\"text\": text}}, params={'taxonomy': 'geotax', 'language': language})\n",
    "\n",
    "geo_categories = []\n",
    "geo_scores = []\n",
    "\n",
    "print (f'{\"CATEGORY\":{27}} {\"ID\":{10}} {\"FREQUENCY\":{8}}')\n",
    "for category in geo_classification.categories:\n",
    "    geo_categories.append(category.label)\n",
    "    geo_scores.append(category.frequency)\n",
    "    print (f'{category.label:{27}} {category.id_:{10}}{category.frequency:{8}}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(geo_categories, geo_scores, color='#66E295')\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Geographic TaxonomyClassification\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! You're an expert in the expert.ai community! \n",
    "\n",
    "Check out other language SDKs available on our [Github page](https://github.com/therealexpertai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
