{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img style=\"float: right;\" src=\"https://docs.expert.ai/logo.png\" width=\"150px\">\n",
    " \n",
    "# My first Notebook with expert.ai Natural Language API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **expert.ai Natural Language API** (https://developer.expert.ai/) parses and \"understands\" large volumes of text.\n",
    "\n",
    "In this section we'll install and play with expert.ai Natural Language API to work with Python, and then introduce some concepts related to Natural Language Processing.\n",
    "\n",
    "You can also download the source code of our Python SDK and this notebook from Github at https://github.com/therealexpertai/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "First, install __expert.ai-nlapi__ library using pip. \n",
    "* https://pypi.org/project/expertai-nlapi/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U expertai-nlapi==1.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, you're ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with expert.ai in Python\n",
    "First you have to setup your account credentials; if you don't have them, get them at https://developer.expert.ai/ui/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"EAI_USERNAME\"] = 'techlab@expertsystem.com'\n",
    "os.environ[\"EAI_PASSWORD\"] = 'N3vershareyourpwd!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play with Python and Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the API supports five languages i.e. English, French, Spanish, Italian and German. You have to define the text you want to process and the language model to use for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expertai.client import ExpertAiClient\n",
    "client = ExpertAiClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Facebook is looking at buying an American startup for $6 million based in Springfield, IL .' \n",
    "language= 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick run\n",
    "Let's start with the fist API, just sending the text. This is how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `disambiguation` analysis returns all the information generated by the Natural Language engine from the text. Let's see in the details the available metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Lemmatization\n",
    "Lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{\"TOKEN\":{20}} {\"LEMMA\":{8}}')\n",
    "\n",
    "for token in document.tokens:\n",
    "    print (f'{text[token.start:token.end]:{20}} {token.lemma:{8}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part of Speech \n",
    "We also looked at the part-of-speech information assigned to each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{\"TOKEN\":{18}} {\"PoS\":{6}}')\n",
    "\n",
    "for token in document.tokens:\n",
    "    print (f'{text[token.start:token.end]:{18}} {token.pos.key:{6}} ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing information\n",
    "The dependency parsing information are available for each token, together with the information about the connected tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{\"TOKEN\":{18}} {\"Dependency label\":{8}}')\n",
    "\n",
    "for token in document.tokens:\n",
    "    print (f'{text[token.start:token.end]:{18}} {token.dependency.label:{4}} ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities\n",
    "Going a step beyond tokens, *named entities* add another layer of context.  Named entities are accessible through the `entities` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'entities'})\n",
    "\n",
    "\n",
    "print (f'{\"ENTITY\":{20}} {\"TYPE\":{10}} {\"TYPE_EXPLAINED\":{10}}')\n",
    "       \n",
    "for entity in document.entities:\n",
    "    print (f'{entity.lemma:{20}} {entity.type_.key:{10}} {entity.type_.description:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can get the open data connected with an entity, i.e `Springfield, IL` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document.entities[1].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in document.knowledge:\n",
    "    if (entry.syncon == document.entities[1].syncon):\n",
    "            for prop in entry.properties:\n",
    "                print (f'{prop.type_:{12}} {prop.value:{30}}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Springfield has been recognized as [Q28515](https://www.wikidata.org/wiki/Q28515) on Wikidata, that is the Q-id for Springfield, IL (i.e.not for Springfield in Vermont o in California)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Elements\n",
    "*Key elements* are identified from the document as main sentences, main keywords, main lemmas and relevant topics; let's focus on the main lemmas of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'relevants'})\n",
    "\n",
    "\n",
    "print (f'{\"LEMMA\":{20}} {\"SCORE\":{5}} ')\n",
    "       \n",
    "for mainlemma in document.main_lemmas:\n",
    "    print (f'{mainlemma.value:{20}} {mainlemma.score:{5}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Let's see how to classify documents according the **IPTC Media Topics Taxonomy**; we're going to use a text that has more textual information and then we'll use the matplot lib to show the categorization result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Strategic acquisitions have been important to the growth of Facebook (FB). \n",
    "Mark Zuckerberg founded the company in 2004, and since then it has acquired scores of companies, \n",
    "ranging from tiny two-person start-ups to well-established businesses such as WhatsApp. For 2019, \n",
    "Facebook reported 2.5 billion monthly active users (MAU) and $70.69 billion in revenue.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "document = client.iptc_media_topics_classification(body={\"document\": {\"text\": text}}, params={'language': language})\n",
    "\n",
    "categories = []\n",
    "scores = []\n",
    "\n",
    "print (f'{\"CATEGORY\":{27}} {\"IPTC ID\":{10}} {\"FREQUENCY\":{8}}')\n",
    "for category in document.categories:\n",
    "    categories.append(category.label)\n",
    "    scores.append(category.frequency)\n",
    "    print (f'{category.label:{27}} {category.id_:{10}}{category.frequency:{8}}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(categories, scores, color='#17a2b8')\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Media Topics Classification\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! You're an expert in the expert.ai community! \n",
    "\n",
    "Check out other language SDKs available on our [Github page](https://github.com/therealexpertai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
